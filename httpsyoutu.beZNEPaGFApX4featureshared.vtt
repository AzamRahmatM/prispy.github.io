WEBVTT


::cue(v[voice="Speaker1"]) { color: white }

1
00:00:11.810 --> 00:00:16.520
In the digital age, data plays a huge role in
our everyday lives.

2
00:00:16.550 --> 00:00:21.380
It's present in lots of obvious ways when
we're shopping online, for example, and have

3
00:00:21.380 --> 00:00:23.210
to type in our name and address.

4
00:00:23.720 --> 00:00:25.940
But data collection can also be less
visible.

5
00:00:26.810 --> 00:00:29.120
Take data brokers, for example.

6
00:00:29.120 --> 00:00:33.380
You've probably never heard of them, but
these businesses specialize in creating

7
00:00:33.380 --> 00:00:35.720
in-depth profiles of individuals for
advertisers.

8
00:00:36.830 --> 00:00:41.090
A single profile might draw on up to 1500
data points.

9
00:00:41.510 --> 00:00:46.400
This can include a person's sexuality,
browsing history, political affiliation, and

10
00:00:46.400 --> 00:00:47.930
even medical records.

11
00:00:47.930 --> 00:00:54.590
One US based data broker, axiom, claims to
have files on 10% of the world's population.

12
00:00:55.300 --> 00:00:57.370
It's not just businesses, of course.

13
00:00:57.400 --> 00:01:02.830
In 2013, Edward Snowden uncovered a vast
regime of mass government surveillance

14
00:01:02.830 --> 00:01:07.540
programs, opening a global conversation
which is still unfolding today.

15
00:01:08.230 --> 00:01:13.360
In this video, we'll take a closer look at
this debate, focusing on the related but

16
00:01:13.360 --> 00:01:15.670
distinct concepts of privacy and data
protection.

17
00:01:16.540 --> 00:01:22.300
By the end, you should have a clear idea of
how these issues differ and overlap, how both

18
00:01:22.300 --> 00:01:27.430
are affected by the digital age, and how you
can engage with companies and governments to

19
00:01:27.430 --> 00:01:29.560
protect and strengthen these rights.

20
00:01:32.730 --> 00:01:36.570
So what are we talking about when we say
privacy and data protection?

21
00:01:36.570 --> 00:01:38.670
Let's take privacy first.

22
00:01:39.210 --> 00:01:44.400
Article 12 of the Universal Declaration of
Human Rights treats privacy as a distinct

23
00:01:44.400 --> 00:01:45.510
human right.

24
00:01:45.510 --> 00:01:51.180
It says that no one shall be subjected to
arbitrary interference with his privacy,

25
00:01:51.180 --> 00:01:53.520
family, home, or correspondence.

26
00:01:53.550 --> 00:01:58.800
Everyone has the right to the protection of
the law against such interference or attacks.

27
00:01:59.250 --> 00:02:00.510
This is simple enough.

28
00:02:00.510 --> 00:02:04.680
Agreeing on what privacy actually means here
has proved harder.

29
00:02:04.980 --> 00:02:09.810
Depending on the context, it can mean the
right to freedom of thought and conscience,

30
00:02:09.810 --> 00:02:14.760
the right to be left alone, the right to
control one's own body, the right to protect

31
00:02:14.760 --> 00:02:20.370
your reputation, the right to a family life,
or the right to a sexuality of your own

32
00:02:20.370 --> 00:02:23.970
definition. There are other ambiguities.

33
00:02:23.970 --> 00:02:27.210
In legal terms, privacy isn't an absolute
right.

34
00:02:27.930 --> 00:02:30.720
This means it can be restricted for certain
reasons.

35
00:02:31.380 --> 00:02:37.800
For example, to protect national security or
public safety, or if it conflicts with other

36
00:02:37.800 --> 00:02:40.680
rights like the right to free expression.

37
00:02:40.680 --> 00:02:45.750
An example could be a public figure invoking
privacy to avoid disclosing their financial

38
00:02:45.750 --> 00:02:49.260
records. And what about data protection?

39
00:02:49.860 --> 00:02:54.510
Contrary to popular belief, it is not the
same thing as privacy.

40
00:02:54.690 --> 00:02:59.790
Privacy is a broad concept, referring to the
conditions which enable a basic foundation of

41
00:02:59.790 --> 00:03:01.740
human dignity and autonomy.

42
00:03:02.100 --> 00:03:04.740
Data protection is more specific.

43
00:03:05.040 --> 00:03:10.650
It's concerned with the ways third parties
handle the information they hold about us,

44
00:03:10.650 --> 00:03:14.430
how it is collected, processed, shared,
stored and used.

45
00:03:14.430 --> 00:03:19.830
In other words, privacy is the big picture
and data protection is one corner of it.

46
00:03:20.510 --> 00:03:23.840
Like privacy, data protection is also
subject to limits.

47
00:03:23.840 --> 00:03:28.130
For example, when a warrant is obtained
allowing law enforcement to access the phone

48
00:03:28.130 --> 00:03:29.570
records of a suspect.

49
00:03:29.870 --> 00:03:34.370
And while data protection is in some ways
more clearly defined than privacy, how it is

50
00:03:34.370 --> 00:03:38.360
applied legally can still vary greatly
depending on which country you're in.

51
00:03:42.760 --> 00:03:48.850
The digital age has created new ways to
collect, access, analyse and use data, often

52
00:03:48.850 --> 00:03:51.460
across multiple borders and jurisdictions.

53
00:03:51.490 --> 00:03:55.450
Unsurprisingly, this poses challenges for
human rights.

54
00:03:56.170 --> 00:04:00.040
One challenge relates to the way companies
use our data.

55
00:04:00.070 --> 00:04:04.000
The internet's business model depends on
people sharing their personal data in

56
00:04:04.000 --> 00:04:08.170
exchange for access to content, services and
social media platforms.

57
00:04:08.470 --> 00:04:13.030
While you might not pay anything upfront to
go on Facebook, they still make money from

58
00:04:13.030 --> 00:04:18.430
you by selling your personal information to
advertisers by clicking Agree to Terms of

59
00:04:18.430 --> 00:04:24.040
Service. Users technically consent to this
model, but in practice virtually no one

60
00:04:24.040 --> 00:04:25.240
actually reads them.

61
00:04:25.240 --> 00:04:29.590
This is a problem because no one knows what
they're really signing up to, which creates

62
00:04:29.590 --> 00:04:31.660
opportunities for misuse.

63
00:04:31.990 --> 00:04:36.400
Another challenge relates to the collection
of personal data by governments.

64
00:04:36.880 --> 00:04:40.870
Technological developments now enable
governments to monitor our conversations,

65
00:04:40.870 --> 00:04:43.510
transactions and the location we visit.

66
00:04:43.960 --> 00:04:49.210
In some countries, including Russia, Brazil,
Australia and South Korea, companies are

67
00:04:49.210 --> 00:04:53.980
legally required to store this data locally
for long periods of time, making it easier

68
00:04:53.980 --> 00:04:55.870
for governments to get information on their
citizens.

69
00:04:56.980 --> 00:05:02.140
These measures are often introduced in the
name of fighting cybercrime and terrorism,

70
00:05:02.440 --> 00:05:04.660
but without adequate protections.

71
00:05:04.660 --> 00:05:10.630
This data can easily be abused to target
dissidents and activists, undermining freedom

72
00:05:10.630 --> 00:05:13.990
of expression and the rights to association
and assembly.

73
00:05:14.670 --> 00:05:16.800
And these are just the technologies we have
now.

74
00:05:17.880 --> 00:05:23.190
Emerging technologies like the Internet of
Things, wearables and artificial intelligence

75
00:05:23.190 --> 00:05:25.410
are likely to pose new challenges to human
rights.

76
00:05:26.070 --> 00:05:29.580
As human rights defenders, we need to be
prepared for these.

77
00:05:33.230 --> 00:05:37.580
There are many bodies and forums where
privacy and data protection issues are

78
00:05:37.580 --> 00:05:39.020
discussed and defined.

79
00:05:39.500 --> 00:05:43.280
National and regional courts have a crucial
role here.

80
00:05:43.460 --> 00:05:48.050
The European Court of Human Rights, for
example, has imposed limits on stop and

81
00:05:48.050 --> 00:05:53.870
search practices by the police and on the
amount of time data can be legally retained.

82
00:05:54.200 --> 00:05:59.780
At the national level, it's common to find a
specific public body responsible for privacy

83
00:05:59.780 --> 00:06:01.010
and data protection.

84
00:06:01.010 --> 00:06:06.500
This can be a specialist post or an
ombudsman, but the extent to which privacy is

85
00:06:06.500 --> 00:06:10.760
defined and protected varies greatly between
different jurisdictions.

86
00:06:10.760 --> 00:06:15.590
For example, there is no clear right to
privacy in the African Charter on Human and

87
00:06:15.590 --> 00:06:16.730
Peoples Rights.

88
00:06:17.390 --> 00:06:22.790
However, there are mechanisms at the
international level following a UN resolution

89
00:06:22.790 --> 00:06:27.650
on the right to privacy in the digital age,
the Human Rights Council has established a

90
00:06:27.650 --> 00:06:33.290
new Special Rapporteur for privacy and
various internet policy forums like the

91
00:06:33.290 --> 00:06:37.970
Internet Governance Forum, the Council of
Europe, the organisation for Economic

92
00:06:37.970 --> 00:06:44.000
Cooperation and Development and conferences
like Hope and Sci Fi also contribute to

93
00:06:44.000 --> 00:06:46.430
shaping the scope of privacy in the digital
age.

94
00:06:47.930 --> 00:06:50.090
And finally, we have companies.

95
00:06:50.090 --> 00:06:55.040
The decisions of companies can also have a
huge impact on data protection and privacy

96
00:06:55.040 --> 00:07:01.130
rights, for example, by building end to end
encryption into their software, as WhatsApp

97
00:07:01.130 --> 00:07:03.500
did in early 2016.

98
00:07:06.160 --> 00:07:10.600
Let's look at two examples of privacy and
data protection in the real world.

99
00:07:10.630 --> 00:07:13.360
First, let's look at the Apple versus FBI
case.

100
00:07:14.560 --> 00:07:20.950
After the 2016 terrorist attacks in the US
city of San Bernardino, the FBI asked Apple

101
00:07:20.950 --> 00:07:24.730
for the information stored on the iPhone of
one of the suspects.

102
00:07:24.760 --> 00:07:30.010
However, Apple's operating system is
encrypted and only accessible through a Pin

103
00:07:30.010 --> 00:07:34.330
code. The FBI asked Apple to modify the
system to let them in.

104
00:07:34.360 --> 00:07:40.240
Apple refused, opening a lively debate on
the right to privacy versus security needs.

105
00:07:40.810 --> 00:07:45.850
The case was almost taken to court, but in
the end, the FBI found a vulnerability to

106
00:07:45.850 --> 00:07:46.960
crack the phone.

107
00:07:46.960 --> 00:07:50.110
In privacy terms, this was a legal setback.

108
00:07:50.440 --> 00:07:54.970
If the case had gone to court, it could have
helped popularize the risks of weakening

109
00:07:54.970 --> 00:07:59.590
encryption for society and establish what
constitutes a legitimate limitation on

110
00:07:59.590 --> 00:08:01.150
privacy by the state.

111
00:08:01.210 --> 00:08:04.450
Next, let's look at surveillance in Kenya.

112
00:08:05.130 --> 00:08:10.380
In Kenya, a combination of invasive
surveillance measures and a lack of adequate

113
00:08:10.380 --> 00:08:17.520
data protection facilitated a crackdown on
civil society in 2013, which was documented

114
00:08:17.520 --> 00:08:19.680
by Peace Brigades International.

115
00:08:20.280 --> 00:08:25.020
Many human rights defenders had their
offices raided, computers hacked and phones

116
00:08:25.020 --> 00:08:26.610
tapped by the government.

117
00:08:27.180 --> 00:08:31.680
One of the ways human rights defenders have
been fighting back is by pushing for the

118
00:08:31.680 --> 00:08:37.170
ratification of Kenya's first data
protection law, long stalled in Parliament.

119
00:08:37.200 --> 00:08:42.450
If implemented properly, this could limit
the worst excesses of state surveillance.

120
00:08:42.690 --> 00:08:47.790
Kenya is by no means the only country to
bring in surveillance legislation justified

121
00:08:47.790 --> 00:08:53.460
by security concerns, but this example is a
good demonstration of how seemingly abstract

122
00:08:53.460 --> 00:08:59.130
restrictions on online privacy can have
physical consequences in the offline world.

123
00:09:04.320 --> 00:09:08.970
So what can human rights defenders do to
protect and strengthen privacy and data

124
00:09:08.970 --> 00:09:14.700
protection? An easy first step is taking
digital security measures yourself.

125
00:09:15.000 --> 00:09:19.890
This can be as simple as using encryption
and anonymity tools and encouraging your

126
00:09:19.890 --> 00:09:21.210
friends to do the same.

127
00:09:21.690 --> 00:09:26.730
Human rights defenders can also advocate for
alternative digital business models, which

128
00:09:26.730 --> 00:09:28.650
aren't based on the extraction and sale of
data.

129
00:09:29.820 --> 00:09:33.270
Economic pressure on the existing model is
already growing.

130
00:09:33.420 --> 00:09:37.980
For example, over the last few years, the
number of users using Adblock software

131
00:09:37.980 --> 00:09:39.780
globally has exploded.

132
00:09:39.810 --> 00:09:44.400
There is evidence that this is already
pushing companies to less invasive

133
00:09:44.400 --> 00:09:46.080
advertising practices.

134
00:09:46.820 --> 00:09:51.770
Engagement in debates at the national and
regional level is, of course, crucial.

135
00:09:51.770 --> 00:09:56.780
Where privacy protections are weak, human
rights defenders need to actively advocate

136
00:09:56.780 --> 00:10:01.550
for stronger ones, and even where they are
stronger, we need to make sure legislation is

137
00:10:01.550 --> 00:10:05.960
keeping up with new technological
developments like the Internet of Things.

138
00:10:06.530 --> 00:10:11.600
Ultimately, if we want things to change,
human rights defenders need to make these

139
00:10:11.600 --> 00:10:16.370
issues accessible and relatable by being
more creative about the way we talk about

140
00:10:16.370 --> 00:10:21.470
them. When people see how data protection
and privacy affects them on a day to day

141
00:10:21.470 --> 00:10:25.910
basis, they may be more inclined to engage
with these concepts.

142
00:10:25.910 --> 00:10:30.650
In the next video, we'll be taking a closer
look at freedom of expression, association,

143
00:10:30.650 --> 00:10:32.180
and peaceful assembly.